# Environment-setup

## Conda Installation
The code was tested on pytorch 2.0 + cu117, xfromers==0.0.18, python=3.9.23, you may need to carefully adjust the version of torch, xfromers and cuda according to your GPUs.
```
conda env create -f environment.yml
```
```
conda activate skelvit
```
```
pip install --no-deps albucore==0.0.24
```
## Detectron2 Installation
To test images â€œin the wild,â€ you need to install the appropriate version of [Detectron2](https://detectron2.readthedocs.io/en/latest/tutorials/install.html) corresponding to your specific GPU, CUDA, and PyTorch configuration.

# Data-preparation

Choose your setup path:
- [Quick Start](#--quick-start): Run the demo in minutes with pre-configured settings
- [Advanced Setup](#--advanced-setup): Deep dive into the method details and custom configurations

<!-- We have uploaded our various datasets to the OneDrive. You can download the required resources from [this link](https://1drv.ms/f/c/89cf0bfd859af8e2/IgCPQSPYHWA1T495bGcBbJCqAVPl4MX0-1-_8G4LdbYRxSA?e=8JHYI7). -->
## ğŸš€ Quick start

### Prerequisites: Download Body Models

Before running the code, you need to download these files:

| Model | Description | Download Link |
|-------|-------------|---------------|
| SKEL | Skeleton body model | [Download](https://skel.is.tue.mpg.de/download.php) |
| J_regressor | Auxiliary joint regressor | [Download](https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy84OWNmMGJmZDg1OWFmOGUyL0lnQ1BRU1BZSFdBMVQ0OTViR2NCYkpDcUFWUGw0TVgwLTEtXzhHNExkYllSeFNBP2U9OEpIWUk3&id=89CF0BFD859AF8E2%21sd823418f601d4f358f796c67016c90aa&cid=89CF0BFD859AF8E2) |
| SMPL | SMPL body model | [Download](https://smpl.is.tue.mpg.de/download.php) |

Once downloaded, organize the files according to the directory structure below:

    data_inputs/
    â””â”€â”€ body_models/
        â”œâ”€â”€ skel/
        â”‚   â”œâ”€â”€ Geometry/
        â”‚   â”œâ”€â”€ sample_motion/
        â”‚   â”œâ”€â”€ bsm.osim
        â”‚   â”œâ”€â”€ changelog_v1.1.1.txt
        â”‚   â”œâ”€â”€ skel_female.pkl
        â”‚   â”œâ”€â”€ skel_male.pkl
        â”‚   â””â”€â”€ tmp.osim
        â”œâ”€â”€ SMPL/
        â”‚   â”œâ”€â”€ SMPL_FEMALE.pkl
        â”‚   â”œâ”€â”€ SMPL_MALE.pkl
        â”‚   â””â”€â”€ SMPL_NEUTRAL.pkl
        â”œâ”€â”€ J_regressor_h36m.pkl
        â”œâ”€â”€ SMPL_TO_J19.pkl
        â”œâ”€â”€ J_regressor_SKEL_mix_MALE.pkl
        â””â”€â”€ J_regressor_SMPL_MALE.pkl

### SKEL-CF Checkpoint
Download **best.pth** from [SKEL-CF](https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy84OWNmMGJmZDg1OWFmOGUyL0lnQ1BRU1BZSFdBMVQ0OTViR2NCYkpDcUFWUGw0TVgwLTEtXzhHNExkYllSeFNBP2U9OEpIWUk3&id=89CF0BFD859AF8E2%21sd823418f601d4f358f796c67016c90aa&cid=89CF0BFD859AF8E2) and place it at data_outputs/exp/ (or wherever you like, but rember to adjust the path in eval.sh and rendering bash shells)

## âš™ï¸ Advanced setup

### Pretrained Camera Model Checkpoint & ViTPose Backbone Checkpoint
Download **backbone** from OneDrive and place it under the data_inputs/ directory. The file structure will be as follows:

    data_inputs/
    â””â”€â”€ backbone/
        â”œâ”€â”€ cam_model_cleaned.ckpt
        â””â”€â”€ vitpose_backbone.pth


### Training images
1. Download the training datasets following the [HSMR](https://github.com/IsshikiHugh/HSMR) instructions.
2. Extract webdataset archives into individual image files.
3. Organize the images according to the directory structure shown below.

```shell
    data_inputs/
    â””â”€â”€ skel-training-images/
        â”œâ”€â”€ aic-train
        â”œâ”€â”€ coco-train
        â”œâ”€â”€ h36m-train
        â”œâ”€â”€ insta-train
        â”œâ”€â”€ mpi-inf-train
        â””â”€â”€ mpii-train
```
### Training labels
Download `skel-training-labels` folder from OneDrive and place it under the `data_inputs/` directory. The file structure should be:

```shell
    data_inputs/
    â””â”€â”€ skel-training-labels/
        â”œâ”€â”€ aic-release_skel.npz
        â”œâ”€â”€ coco-release_skel.npz
        â”œâ”€â”€ h36m-release_skel.npz
        â”œâ”€â”€ insta1-release_skel.npz
        â”œâ”€â”€ insta2-release_skel.npz
        â”œâ”€â”€ mpi-inf-release_skel.npz
        â””â”€â”€ mpii-release_skel.npz
```

# Evaluation Data
We evaluated our method on several benchmark datasets, including COCO, 3DPW, EMDB, SPEC-SYN and H36M used by [HSMR](https://github.com/IsshikiHugh/HSMR) or [CameraHMR](https://camerahmr.is.tue.mpg.de/). We also tested on the yoga action dataset [MOYO](https://moyo.is.tue.mpg.de/), as well as its more challenging subset, MOYO-HARD. Some datasets are relatively difficult to process, so just take your time.

## Evaluation labels
Download the evaluation labels from [SKEL-CF](https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy84OWNmMGJmZDg1OWFmOGUyL0lnQ1BRU1BZSFdBMVQ0OTViR2NCYkpDcUFWUGw0TVgwLTEtXzhHNExkYllSeFNBP2U9OEpIWUk3&id=89CF0BFD859AF8E2%21sd823418f601d4f358f796c67016c90aa&cid=89CF0BFD859AF8E2)
```shell
    data_inputs/
    â””â”€â”€ skel-evaluation-labels/
        â”œâ”€â”€ moyo_hard.npz
        â”œâ”€â”€ moyo_v2.npz
        â”œâ”€â”€ h36m_val_p2.npz
        â”œâ”€â”€ coco_val.npz
        â”œâ”€â”€ spec_test.npz
        â”œâ”€â”€ 3dpw_test.npz
        â””â”€â”€ emdb_test.npz
```    

## Evaluation Images
Download the evaluation images from the following sources:
- [3DPW](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
- [EMDB](https://eth-ait.github.io/emdb/)
- [COCO](https://cocodataset.org/#download)
- [SPEC-SYN](https://spec.is.tue.mpg.de/)
```shell
    data_inputs/
    â””â”€â”€ skel-evaluation-data/
        â”œâ”€â”€ 3dpw
        â”œâ”€â”€ coco
        â”œâ”€â”€ emdb
        â”œâ”€â”€ h36m-select
        â”œâ”€â”€ moyo
        â””â”€â”€ spec-syn
```
